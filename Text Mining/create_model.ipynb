{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json as js\n",
    "import pandas as pd\n",
    "import re # El paquete para tratar texto. Expresiones regulares\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Vectorizador de palabras y DTM\n",
    "from sklearn.decomposition import LatentDirichletAllocation # Modelo de LDA\n",
    "from scipy.sparse import csr_matrix # Para tratar Sparse Matrix\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud #importo la funci칩n\n",
    "import time\n",
    "import datetime\n",
    "# LDA, tSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.palettes import all_palettes\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colors.ListedColormap at 0x222262e3c18>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model\n",
    "Is a generative statistical model that allows sets of observations to be explained by unobserved groups and in this way it can explained why some of the observations are similar between themselves. \n",
    "\n",
    "## Process \n",
    "\n",
    "### Pre Process Data\n",
    "\n",
    "- Process the documents that will be used, get them all in a standard form\n",
    "- Eliminate very short articles \n",
    "- Beware that some metadata or unicode characters could slip.\n",
    "\n",
    "#### Words\n",
    "\n",
    "Since LDA uses a bag of words methodology is very importatn to have the right words in the document, would want to eliminate very common words since they might not carry a very special meaning, or words that are only use once. We could also check for different things or focus on different aspects:\n",
    "\n",
    "- **wordlength** (number of letters in the word) or (n-gram)\n",
    "- **Stoplist** (Words that don't add much meaning to the document, general terms, )\n",
    "- **Lemmatization** (interpretate as equal all the different conjugations of the same verb)\n",
    "- **Parts of Speech**:\n",
    "    + Noun\n",
    "    + Verb\n",
    "    + Adverb\n",
    "    + Adjective\n",
    "    + Preposition\n",
    "    \n",
    "### The Model\n",
    "\n",
    "In the following link we can find a very good description about the intuition in the model and what it tries to do. \n",
    "https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_word_distribution(model, count_vectorizer):\n",
    "    \"\"\" This function recieves an lda_model and a count vectorizer as input and returns a the word probability distribution per topic.\n",
    "    This is the probability that a word belongs to a certain topic\"\"\"\n",
    "    num_topics,num_words= model.components_.shape\n",
    "    topicos=[\"Topic #\"+str(i) for i in range(0,num_topics)]\n",
    "    matrix=pd.DataFrame(model.components_, index=topicos, columns=count_vectorizer.get_feature_names())\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_words_per_topic(model, count_vectorizer, n_words):\n",
    "    \"\"\" This function returns the most important words per topic, according to its inputs: an LDA Model, a Count Vectorizer,\n",
    "    and the number of words wanted\n",
    "    \"\"\"\n",
    "    num_topics,num_words= model.components_.shape\n",
    "    print(num_topics)\n",
    "    words = count_vectorizer.get_feature_names() # extraigo las palabras del modelo\n",
    "    topicos=[\"Topic #\"+str(i) for i in range(0,num_topics)]\n",
    "    resultado=pd.DataFrame(index=topicos, columns=[\"Word\"+str(i) for i in range(1,n_words+1) ])\n",
    "    resultado.fillna(\" \")\n",
    "    for topic in range(0,len(topicos)):\n",
    "        resultado.iloc[topic,:]=[words[j] for j in model.components_[topic].argsort()[-n_words - 1:-1]]\n",
    "    return(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(date_init, date_end, k_topics, n_vocab=350):\n",
    "    \"\"\" This function recieves a start date, end date, number of topics and a length of voacbulary. All of this parameters \n",
    "    will be used to create an LDA model according to those characteristics. An LDA model that only considerates news from the period\n",
    "    of time selected, with k topics and vocalubary of length n_vocab.\n",
    "    \"\"\"\n",
    "   #Se leen los archivos especificos, para el periodo de tiempo especificado.\n",
    "    lemmatized_files=os.listdir('C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales'+\"\\\\Text Mining\\\\process_text\")\n",
    "    lemmatized_files=[k for k in lemmatized_files if len(re.findall(\"E.+\", k)) is not 0 ]\n",
    "    lemmatized_files=pd.DataFrame({\"lemmatized_files\":lemmatized_files})\n",
    "    lemmatized_files_dates=[re.findall(\"[0-9-]+\", j)[0] for j in lemmatized_files[\"lemmatized_files\"]]\n",
    "    lemmatized_files_dates=pd.to_datetime(lemmatized_files_dates, format=\"%Y-%m-%d\")\n",
    "    lemmatized_files[\"dates\"]=lemmatized_files_dates\n",
    "    date_init=pd.to_datetime(date_init)\n",
    "    date_end=pd.to_datetime(date_end)\n",
    "    \n",
    "    selected_files=lemmatized_files.loc[(date_init<=lemmatized_files_dates)&(date_end>=lemmatized_files_dates),\"lemmatized_files\"]\n",
    "    #Se construye el tf, en base a ellos\n",
    "    df=pd.DataFrame({\"categoria\":[], \"contenido\":[],\"link\":[], \"titulo\":[], \"process_time\":[]})\n",
    "    for file in selected_files:\n",
    "        with open('C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales'+\"\\\\Text Mining\\\\process_text\\\\\"+file, \"r\", encoding=\"utf-8\") as t:\n",
    "            info_dict =js.load(t)\n",
    "            temp=pd.DataFrame(info_dict)\n",
    "            df=df.append(temp)\n",
    "    df.shape\n",
    "    \n",
    "    #Cargo las stopwords\n",
    "        # Now we load the stop words from a json file\n",
    "    with open('C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales' + \"\\\\Text Mining\\\\\"+\"spanish_stopwords.json\", 'r' , encoding='utf-8') as d:\n",
    "            stop_words=js.load(d)\n",
    "    stop_words=stop_words[\"words\"]\n",
    "    \n",
    "        # Now we create the document-word matrix, where each row is a document and each column is a word\n",
    "    # m치ximo tama침o de vocabulario\n",
    "    #We define the parameters that will be applied to the model creating an instance of it\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.8, min_df=2, max_features=n_vocab, stop_words=stop_words, ngram_range=(1,3)) # Al igual que un modelo, defino el objeto que construir치 la matriz\n",
    "    #We applied the model to a set of documents\n",
    "    tf = tf_vectorizer.fit_transform(df.contenido) # Aplico el objeto a un conjunto de textos\n",
    "    #We get the vocabulary\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names() # Veo el vocabulario\n",
    "    # Creamos el MODELO LDA\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=k_topics, max_iter=20,doc_topic_prior=0.1, topic_word_prior=0.1, n_jobs=-1,random_state=23) # Construyo el objeto que es el modelo\n",
    "    lda_fit=lda.fit(tf)\n",
    "    lda_output=lda_fit.transform(tf)\n",
    "    docs=['doc'+str(i) for i in range(lda_output.shape[0])] # Nombres de filas\n",
    "    topics=['topic '+str(i) for i in range(lda_output.shape[1])] # Nombres de columnas\n",
    "    lda_output=pd.DataFrame(lda_output, index=docs, columns=topics)\n",
    "    return (lda_fit, lda_output, df,tf_vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(lda_output,df):\n",
    "    \"\"\" This functions recieves the output of an lda_model, which is a probability distribution of the articles in the \n",
    "    k-dimensional topic space and reduces its dimensionality but trying to keep its similiraties and differences in a new \n",
    "    2-dimensional space. \n",
    "    \"\"\"\n",
    "    tsne = TSNE(random_state=2017, perplexity=30, early_exaggeration=120)\n",
    "    num_docs,num_topics=lda_output.shape\n",
    "    \n",
    "    data=np.array(lda_output.iloc[:,0:num_topics])\n",
    "    \n",
    "    embedding = tsne.fit_transform(data)\n",
    "    embedding = pd.DataFrame(embedding, columns=['x_tsne','y_tsne'])\n",
    "    embedding['topico_dominante'] =  np.argmax(data, axis=1)\n",
    "    \n",
    "    df2=pd.concat([df.reset_index(drop=True),embedding], axis=1)\n",
    "\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(lda_output,df2):\n",
    "        \"\"\" This functions recieves the output of an lda_model, which is a probability distribution of the articles in the \n",
    "    k-dimensional topic space and reduces its dimensionality by projecting the 2 principal component vectors into a new orthogonal\n",
    "    2 dimensional space.     \"\"\"\n",
    "        pca = PCA(n_components=2)\n",
    "        num_docs,num_topics=lda_output.shape\n",
    "        data=np.array(lda_output.iloc[:,0:num_topics])\n",
    "        pca_result = pd.DataFrame(pca.fit_transform(data),columns=[\"x_pca\", \"y_pca\"])\n",
    "        df3=pd.concat([df2.reset_index(drop=True),pca_result], axis=1)\n",
    "        return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(tf_vectorizer,filename,df):\n",
    "    cloud=WordCloud(background_color='white', width=375, height=375, max_words=50, max_font_size=225, stopwords=tf_vectorizer.get_stop_words(), colormap=cm.get_cmap(name=\"inferno\", lut=None),random_state=123) # Construyo el generador de la nube\n",
    "    cloud.generate('.'.join(list(df.contenido))) # Genero la nube\n",
    "    cloud.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_escenario(list_fechas, num_topics):\n",
    "    \"\"\"This function creates a scenario. This scenario is one hypothetical construction of one of the many ways the model\n",
    "    could be constructed taking into account possible dates, and number of topics.\"\"\"\n",
    "    model, output, df, tf=lda_model(list_fechas[0],list_fechas[1], num_topics)\n",
    "    tsne_df=tsne(output, df)\n",
    "    df_tot=pca(output, tsne_df)\n",
    "    topic_word_df=topics_word_distribution(model,tf)\n",
    "    \n",
    "    return (df_tot, output,topic_word_df,tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_escenarios(lista_fechas):\n",
    "    for rango in lista_fechas:\n",
    "        print(rango)\n",
    "        os.mkdir(\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Proyectos-Personales\\\\Dash App\\\\Escenarios\\\\\"+\n",
    "                 str(rango[0])+\"__\"+str(rango[1]))\n",
    "        for num_topics in range(4,14,2):\n",
    "            print(num_topics)\n",
    "            _dir_=\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Proyectos-Personales\\\\Dash App\\\\Escenarios\\\\\"+str(rango[0])+\"__\"+str(rango[1])+\"\\\\\"+str(num_topics)+\"\\\\\"\n",
    "            os.mkdir(_dir_)\n",
    "            df_tot, lda_output, topic_word_df,tf=crear_escenario(rango, num_topics)\n",
    "            generate_word_cloud(tf,_dir_+\"word_cloud.jpg\",df_tot)\n",
    "            df_tot.to_csv(_dir_+\"df_tot.csv\", encoding=\"utf-8\")\n",
    "            lda_output.to_csv(_dir_+\"lda_output.csv\", encoding=\"utf-8\")\n",
    "            topic_word_df.to_csv(_dir_+\"topic_word_df.csv\", encoding=\"utf-8\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a range of dates, we create a list of possible combinations of dates, to create a scenario for each possible combination of \n",
    "# date, and number of topics.\n",
    "fecha_inicial =datetime.date(2020,4,13)\n",
    "fechas_iniciales=[fecha_inicial+datetime.timedelta(14*i) for i in range(0,8)]\n",
    "fecha_final = datetime.date(2020,4,26)\n",
    "fechas_finales=[fecha_final+datetime.timedelta(14*i) for i in range(0,8)]\n",
    "lista_fechas=[]\n",
    "for fecha_i in fechas_iniciales:\n",
    "    for fecha_f in fechas_finales:\n",
    "        fecha_temp=[]\n",
    "        if fecha_i<fecha_f:\n",
    "            fecha_temp.append(fecha_i)\n",
    "            fecha_temp.append(fecha_f)\n",
    "            lista_fechas.append(fecha_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.date(2020, 4, 13), datetime.date(2020, 4, 26)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 5, 10)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 5, 24)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 6, 7)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 6, 21)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 4, 13), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 5, 10)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 5, 24)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 6, 7)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 6, 21)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 4, 27), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 5, 24)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 6, 7)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 6, 21)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 5, 11), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 5, 25), datetime.date(2020, 6, 7)],\n",
       " [datetime.date(2020, 5, 25), datetime.date(2020, 6, 21)],\n",
       " [datetime.date(2020, 5, 25), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 5, 25), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 5, 25), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 6, 8), datetime.date(2020, 6, 21)],\n",
       " [datetime.date(2020, 6, 8), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 6, 8), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 6, 8), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 6, 22), datetime.date(2020, 7, 5)],\n",
       " [datetime.date(2020, 6, 22), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 6, 22), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 7, 6), datetime.date(2020, 7, 19)],\n",
       " [datetime.date(2020, 7, 6), datetime.date(2020, 8, 2)],\n",
       " [datetime.date(2020, 7, 20), datetime.date(2020, 8, 2)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2020, 4, 13), datetime.date(2020, 4, 26)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 5, 10)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 5, 24)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 6, 7)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 6, 21)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 13), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 5, 10)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 5, 24)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 6, 7)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 6, 21)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 4, 27), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 5, 24)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 6, 7)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 6, 21)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 11), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 25), datetime.date(2020, 6, 7)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 25), datetime.date(2020, 6, 21)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 25), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 25), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 5, 25), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 8), datetime.date(2020, 6, 21)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 8), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 8), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 8), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 22), datetime.date(2020, 7, 5)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 22), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 6, 22), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 7, 6), datetime.date(2020, 7, 19)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 7, 6), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "[datetime.date(2020, 7, 20), datetime.date(2020, 8, 2)]\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "crear_escenarios(lista_fechas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5]\n",
    "a[0:-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
