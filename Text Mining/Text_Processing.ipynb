{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json as js\n",
    "import pandas as pd\n",
    "import re # El paquete para tratar texto. Expresiones regulares\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora_tokenizer.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora_mwt_expander.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora.pretrain.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora_lemmatizer.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\usuario\\\\stanfordnlp_resources\\\\es_ancora_models\\\\es_ancora.pretrain.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "stfnlp=stanfordnlp.Pipeline(lang=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Proyectos-Personales\")\n",
    "json_files=os.listdir(os.getcwd() + \"\\\\News Retriever\\\\El_Tiempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news_El_Tiempo_2020-04-13.json',\n",
       " 'news_El_Tiempo_2020-04-15.json',\n",
       " 'news_El_Tiempo_2020-04-16.json',\n",
       " 'news_El_Tiempo_2020-04-17.json',\n",
       " 'news_El_Tiempo_2020-04-18.json',\n",
       " 'news_El_Tiempo_2020-04-19.json',\n",
       " 'news_El_Tiempo_2020-04-20.json',\n",
       " 'news_El_Tiempo_2020-04-21.json',\n",
       " 'news_El_Tiempo_2020-04-22.json',\n",
       " 'news_El_Tiempo_2020-04-23.json',\n",
       " 'news_El_Tiempo_2020-04-24.json',\n",
       " 'news_El_Tiempo_2020-04-25.json',\n",
       " 'news_El_Tiempo_2020-04-26.json',\n",
       " 'news_El_Tiempo_2020-04-27.json',\n",
       " 'news_El_Tiempo_2020-04-28.json',\n",
       " 'news_El_Tiempo_2020-04-29.json',\n",
       " 'news_El_Tiempo_2020-04-30.json',\n",
       " 'news_El_Tiempo_2020-05-01.json',\n",
       " 'news_El_Tiempo_2020-05-02.json',\n",
       " 'news_El_Tiempo_2020-05-03.json',\n",
       " 'news_El_Tiempo_2020-05-04.json',\n",
       " 'news_El_Tiempo_2020-05-05.json',\n",
       " 'news_El_Tiempo_2020-05-06.json',\n",
       " 'news_El_Tiempo_2020-05-07.json',\n",
       " 'news_El_Tiempo_2020-05-09.json',\n",
       " 'news_El_Tiempo_2020-05-10.json',\n",
       " 'news_El_Tiempo_2020-05-11.json',\n",
       " 'news_El_Tiempo_2020-05-13.json',\n",
       " 'news_El_Tiempo_2020-05-14.json',\n",
       " 'news_El_Tiempo_2020-05-16.json',\n",
       " 'news_El_Tiempo_2020-05-17.json',\n",
       " 'news_El_Tiempo_2020-05-20.json',\n",
       " 'news_El_Tiempo_2020-05-21.json',\n",
       " 'news_El_Tiempo_2020-05-22.json',\n",
       " 'news_El_Tiempo_2020-05-25.json',\n",
       " 'news_El_Tiempo_2020-05-26.json',\n",
       " 'news_El_Tiempo_2020-06-02.json',\n",
       " 'news_El_Tiempo_2020-06-03.json',\n",
       " 'news_El_Tiempo_2020-06-15.json',\n",
       " 'news_El_Tiempo_2020-06-16.json',\n",
       " 'news_El_Tiempo_2020-06-17.json',\n",
       " 'news_El_Tiempo_2020-06-18.json',\n",
       " 'news_El_Tiempo_2020-06-20.json',\n",
       " 'news_El_Tiempo_2020-06-22.json',\n",
       " 'news_El_Tiempo_2020-06-23.json',\n",
       " 'news_El_Tiempo_2020-06-24.json',\n",
       " 'news_El_Tiempo_2020-06-26.json',\n",
       " 'news_El_Tiempo_2020-06-27.json',\n",
       " 'news_El_Tiempo_2020-06-28.json',\n",
       " 'news_El_Tiempo_2020-06-29.json',\n",
       " 'news_El_Tiempo_2020-06-30.json',\n",
       " 'news_El_Tiempo_2020-07-01.json',\n",
       " 'news_El_Tiempo_2020-07-02.json',\n",
       " 'news_El_Tiempo_2020-07-04.json',\n",
       " 'news_El_Tiempo_2020-07-05.json',\n",
       " 'news_El_Tiempo_2020-07-06.json',\n",
       " 'news_El_Tiempo_2020-07-07.json',\n",
       " 'news_El_Tiempo_2020-07-08.json',\n",
       " 'news_El_Tiempo_2020-07-09.json',\n",
       " 'news_El_Tiempo_2020-07-10.json',\n",
       " 'news_El_Tiempo_2020-07-12.json',\n",
       " 'news_El_Tiempo_2020-07-13.json',\n",
       " 'news_El_Tiempo_2020-07-16.json',\n",
       " 'news_El_Tiempo_2020-07-17.json',\n",
       " 'news_El_Tiempo_2020-07-18.json',\n",
       " 'news_El_Tiempo_2020-07-20.json',\n",
       " 'news_El_Tiempo_2020-07-21.json',\n",
       " 'news_El_Tiempo_2020-07-22.json',\n",
       " 'news_El_Tiempo_2020-07-23.json',\n",
       " 'news_El_Tiempo_2020-07-24.json',\n",
       " 'news_El_Tiempo_2020-07-25.json',\n",
       " 'news_El_Tiempo_2020-07-26.json',\n",
       " 'news_El_Tiempo_2020-07-27.json',\n",
       " 'news_El_Tiempo_2020-07-29.json',\n",
       " 'news_El_Tiempo_2020-07-30.json',\n",
       " 'news_El_Tiempo_2020-08-01.json',\n",
       " 'news_El_Tiempo_2020-08-02.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df):\n",
    "    \n",
    "    ## REVISAR QUE ERA LO QUE ELIMINA Juan Sebastian en la clase de 11 del taller de python\n",
    "    #Turn all into lowercase\n",
    "    df.contenido=df.contenido.str.lower() \n",
    "    # Eliminate all the content inside a parenthesis\n",
    "    df.contenido=df.contenido.str.replace(r'\\([^)]*\\)\\\"', \" \")\n",
    "    # Unicode character found in the documents\n",
    "    df.contenido=df.contenido.str.replace(\"\\xa0\", \" \")\n",
    "    # Unicode character found in the documents\n",
    "    df.contenido=df.contenido.str.replace(\"\\u200b\", \" \")\n",
    "    df.contenido=df.contenido.str.replace('\"',' ')\n",
    "    # Eliminate all the @__name__\n",
    "    df.contenido=df.contenido.str.replace(\"@[a-z]+\", \" \")\n",
    "    #Turn all possible references of Coronavirus into one\n",
    "    df.contenido=df.contenido.str.replace(\"(covid|corona virus)\", \"coronavirus\")\n",
    "    # Eiminate punctuation signs\n",
    "    df.contenido=df.contenido.str.replace('[,\\.%!?\\-!?\\n\\)\\(\\r]', ' ') \n",
    "    # Eliminate numbers\n",
    "    df.contenido=df.contenido.str.replace('[0-9]', ' ') # Quito n√∫meros\n",
    "    # Eliminate the name of the newspaper from the documents\n",
    "    df.contenido=df.contenido.str.replace('eltiempo', '')\n",
    "    # Eliminate the excess of whitespaces\n",
    "    df.contenido=df.contenido.str.replace('  +', ' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatizer(df):   \n",
    "    df[\"process_time\"]=0\n",
    "    for i in range(0,len(df[\"contenido\"])):\n",
    "        start=time.time()\n",
    "        print(i)\n",
    "        num_palabras=len(df.loc[i,\"contenido\"].split(\" \"))\n",
    "        if  num_palabras < 4000:\n",
    "            doc= stfnlp(str(df.loc[i,\"contenido\"]))\n",
    "            df.loc[i,\"contenido\"]=\" \".join([word.lemma for sentence in doc.sentences for word in sentence.words if word.lemma is not None])\n",
    "            end=time.time()\n",
    "            df.loc[i,\"process_time\"]=end-start\n",
    "        else:\n",
    "            palabras=df.loc[i, \"contenido\"].split(\" \")\n",
    "            max_words=3300\n",
    "            iteraciones= len(palabras)/max_words\n",
    "            lemmatized_doc=\" \"\n",
    "            for j in range(0,int(iteraciones)+1):\n",
    "                last_j=min((j+1)*max_words, len(palabras))\n",
    "                first_j=max(0,(j*max_words))\n",
    "                content=\" \".join(palabras[first_j:last_j])\n",
    "                doc= stfnlp(content)\n",
    "                lemmatized_doc=\" \".join([word.lemma for sentence in doc.sentences for word in sentence.words if word.lemma is not None])\n",
    "            df.loc[i,\"contenido\"]= lemmatized_doc                  \n",
    "            end=time.time()\n",
    "            df.loc[i,\"process_time\"]=end-start\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processer_step_1(df):\n",
    "    df=df[(df[\"contenido\"]!='') & (df[\"contenido\"]!=' ')]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df=text_cleaning(df)\n",
    "    df=text_lemmatizer(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file news_El_Tiempo_2020-04-13.json was already processed\n",
      "The file news_El_Tiempo_2020-04-15.json was already processed\n",
      "The file news_El_Tiempo_2020-04-16.json was already processed\n",
      "The file news_El_Tiempo_2020-04-17.json was already processed\n",
      "The file news_El_Tiempo_2020-04-18.json was already processed\n",
      "The file news_El_Tiempo_2020-04-19.json was already processed\n",
      "The file news_El_Tiempo_2020-04-20.json was already processed\n",
      "The file news_El_Tiempo_2020-04-21.json was already processed\n",
      "The file news_El_Tiempo_2020-04-22.json was already processed\n",
      "The file news_El_Tiempo_2020-04-23.json was already processed\n",
      "The file news_El_Tiempo_2020-04-24.json was already processed\n",
      "The file news_El_Tiempo_2020-04-25.json was already processed\n",
      "The file news_El_Tiempo_2020-04-26.json was already processed\n",
      "The file news_El_Tiempo_2020-04-27.json was already processed\n",
      "The file news_El_Tiempo_2020-04-28.json was already processed\n",
      "The file news_El_Tiempo_2020-04-29.json was already processed\n",
      "The file news_El_Tiempo_2020-04-30.json was already processed\n",
      "The file news_El_Tiempo_2020-05-01.json was already processed\n",
      "The file news_El_Tiempo_2020-05-02.json was already processed\n",
      "The file news_El_Tiempo_2020-05-03.json was already processed\n",
      "The file news_El_Tiempo_2020-05-04.json was already processed\n",
      "The file news_El_Tiempo_2020-05-05.json was already processed\n",
      "The file news_El_Tiempo_2020-05-06.json was already processed\n",
      "The file news_El_Tiempo_2020-05-07.json was already processed\n",
      "The file news_El_Tiempo_2020-05-09.json was already processed\n",
      "The file news_El_Tiempo_2020-05-10.json was already processed\n",
      "The file news_El_Tiempo_2020-05-11.json was already processed\n",
      "The file news_El_Tiempo_2020-05-13.json was already processed\n",
      "The file news_El_Tiempo_2020-05-14.json was already processed\n",
      "The file news_El_Tiempo_2020-05-16.json was already processed\n",
      "The file news_El_Tiempo_2020-05-17.json was already processed\n",
      "The file news_El_Tiempo_2020-05-20.json was already processed\n",
      "The file news_El_Tiempo_2020-05-21.json was already processed\n",
      "The file news_El_Tiempo_2020-05-22.json was already processed\n",
      "The file news_El_Tiempo_2020-05-25.json was already processed\n",
      "The file news_El_Tiempo_2020-05-26.json was already processed\n",
      "The file news_El_Tiempo_2020-06-02.json was already processed\n",
      "The file news_El_Tiempo_2020-06-03.json was already processed\n",
      "The file news_El_Tiempo_2020-06-15.json was already processed\n",
      "The file news_El_Tiempo_2020-06-16.json was already processed\n",
      "The file news_El_Tiempo_2020-06-17.json was already processed\n",
      "The file news_El_Tiempo_2020-06-18.json was already processed\n",
      "The file news_El_Tiempo_2020-06-20.json was already processed\n",
      "The file news_El_Tiempo_2020-06-22.json was already processed\n",
      "The file news_El_Tiempo_2020-06-23.json was already processed\n",
      "The file news_El_Tiempo_2020-06-24.json was already processed\n",
      "The file news_El_Tiempo_2020-06-26.json was already processed\n",
      "The file news_El_Tiempo_2020-06-27.json was already processed\n",
      "The file news_El_Tiempo_2020-06-28.json was already processed\n",
      "The file news_El_Tiempo_2020-06-29.json was already processed\n",
      "The file news_El_Tiempo_2020-06-30.json was already processed\n",
      "The file news_El_Tiempo_2020-07-01.json was already processed\n",
      "The file news_El_Tiempo_2020-07-02.json was already processed\n",
      "The file news_El_Tiempo_2020-07-04.json was already processed\n",
      "The file news_El_Tiempo_2020-07-05.json was already processed\n",
      "The file news_El_Tiempo_2020-07-06.json was already processed\n",
      "The file news_El_Tiempo_2020-07-07.json was already processed\n",
      "The file news_El_Tiempo_2020-07-08.json was already processed\n",
      "The file news_El_Tiempo_2020-07-09.json was already processed\n",
      "The file news_El_Tiempo_2020-07-10.json was already processed\n",
      "The file news_El_Tiempo_2020-07-12.json was already processed\n",
      "The file news_El_Tiempo_2020-07-13.json was already processed\n",
      "The file news_El_Tiempo_2020-07-16.json was already processed\n",
      "The file news_El_Tiempo_2020-07-17.json was already processed\n",
      "The file news_El_Tiempo_2020-07-18.json was already processed\n",
      "The file news_El_Tiempo_2020-07-20.json was already processed\n",
      "The file news_El_Tiempo_2020-07-21.json was already processed\n",
      "The file news_El_Tiempo_2020-07-22.json was already processed\n",
      "The file news_El_Tiempo_2020-07-23.json was already processed\n",
      "The file news_El_Tiempo_2020-07-24.json was already processed\n",
      "The file news_El_Tiempo_2020-07-25.json was already processed\n",
      "The file news_El_Tiempo_2020-07-26.json was already processed\n",
      "The file news_El_Tiempo_2020-07-27.json was already processed\n",
      "El_Tiempo_2020-07-29_lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\usuario\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "El_Tiempo_2020-07-30_lemmatized\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "El_Tiempo_2020-08-01_lemmatized\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "El_Tiempo_2020-08-02_lemmatized\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "for file in json_files:\n",
    "    if file != \".ipynb_checkpoints\":\n",
    "        new_file_name=re.findall(\"(?<=news_).+(?=.json)\", file)[0]+\"_lemmatized\"\n",
    "        v=new_file_name+\".json\"\n",
    "        #dir options:\n",
    "        #\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Dash App\\\\dash_app_deployment\\\\data\"\n",
    "        #\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Proyectos-Personales\\\\Dash App\\\\data\"\n",
    "        #\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Text Mining\\\\process_text\"\n",
    "        _dir_=\"C:\\\\Users\\\\usuario\\\\Desktop\\\\Universidad\\\\Proyectos Personales\\\\Text Mining\\\\process_text\"\n",
    "        json_lemmatized_files=os.listdir(_dir_)\n",
    "        if v not in json_lemmatized_files:\n",
    "            print(new_file_name)\n",
    "            with open(os.getcwd() + \"\\\\News Retriever\\\\El_Tiempo\\\\\"+file, 'r') as d:\n",
    "                info_dict =js.load(d)\n",
    "                df=pd.DataFrame(info_dict)\n",
    "            df=text_processer_step_1(df)\n",
    "            with open(_dir_+\"\\\\\"+new_file_name+\".json\", 'w') as f:\n",
    "                js.dump(df.to_dict(), f)\n",
    "        else:\n",
    "            print(\"The file \"+file+\" was already processed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
